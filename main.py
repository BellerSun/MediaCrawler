# å£°æ˜Žï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŽŸåˆ™ï¼š
# 1. ä¸å¾—ç”¨äºŽä»»ä½•å•†ä¸šç”¨é€”ã€‚
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚
# 4. åº”åˆç†æŽ§åˆ¶è¯·æ±‚é¢‘çŽ‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚
# 5. ä¸å¾—ç”¨äºŽä»»ä½•éžæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŽŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚


import asyncio
import sys
from typing import Optional

import cmd_arg
import config
import db
from base.base_crawler import AbstractCrawler
from media_platform.bilibili import BilibiliCrawler
from media_platform.douyin import DouYinCrawler
from media_platform.kuaishou import KuaishouCrawler
from media_platform.tieba import TieBaCrawler
from media_platform.weibo import WeiboCrawler
from media_platform.xhs import XiaoHongShuCrawler
from media_platform.zhihu import ZhihuCrawler
from utils.browser_fix import BrowserAutoFixer


class CrawlerFactory:
    CRAWLERS = {
        "xhs": XiaoHongShuCrawler,
        "dy": DouYinCrawler,
        "ks": KuaishouCrawler,
        "bili": BilibiliCrawler,
        "wb": WeiboCrawler,
        "tieba": TieBaCrawler,
        "zhihu": ZhihuCrawler,
    }

    @staticmethod
    def create_crawler(platform: str) -> AbstractCrawler:
        crawler_class = CrawlerFactory.CRAWLERS.get(platform)
        if not crawler_class:
            raise ValueError(
                "Invalid Media Platform Currently only supported xhs or dy or ks or bili ..."
            )
        return crawler_class()


crawler: Optional[AbstractCrawler] = None


# auto_fix_browser_issue åŠŸèƒ½å·²ç§»åˆ° utils.browser_fix.BrowserAutoFixer ç±»ä¸­


async def main():
    # Init crawler
    global crawler

    # parse cmd
    await cmd_arg.parse_cmd()

    # init db
    if config.SAVE_DATA_OPTION in ["db", "sqlite"]:
        await db.init_db()

    crawler = CrawlerFactory.create_crawler(platform=config.PLATFORM)
    await crawler.start()


async def cleanup():
    if crawler:
        await crawler.close()
    if config.SAVE_DATA_OPTION in ["db", "sqlite"]:
        await db.close()


if __name__ == "__main__":
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            asyncio.get_event_loop().run_until_complete(main())
            break  # æˆåŠŸæ‰§è¡Œï¼Œé€€å‡ºå¾ªçŽ¯
            
        except KeyboardInterrupt:
            print("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
            break
            
        except Exception as e:
            error_message = str(e)
            print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {error_message}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æµè§ˆå™¨ç›¸å…³é”™è¯¯
            is_browser_error = BrowserAutoFixer.is_browser_error(error_message)
            
            if is_browser_error and retry_count < max_retries - 1:
                print(f"\nðŸ¤– æ£€æµ‹åˆ°æµè§ˆå™¨é—®é¢˜ï¼ˆç¬¬ {retry_count + 1} æ¬¡å¤±è´¥ï¼‰ï¼Œæ­£åœ¨è‡ªåŠ¨å¤„ç†...")
                
                # è‡ªåŠ¨æ¸…ç†
                if BrowserAutoFixer.auto_fix():
                    retry_count += 1
                    print(f"\nðŸ”„ å‡†å¤‡é‡è¯•... (å°è¯• {retry_count + 1}/{max_retries})")
                    print("â³ ç­‰å¾… 3 ç§’åŽé‡æ–°å¯åŠ¨...")
                    
                    # æ¸…ç†å½“å‰çˆ¬è™«å®žä¾‹
                    try:
                        if crawler:
                            asyncio.get_event_loop().run_until_complete(crawler.close())
                            crawler = None
                    except:
                        pass
                    
                    # ç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    import time
                    time.sleep(3)
                    continue
                else:
                    print("âŒ è‡ªåŠ¨æ¸…ç†å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å¤„ç†")
                    break
            else:
                # éžæµè§ˆå™¨é”™è¯¯æˆ–é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™
                if is_browser_error:
                    print(f"\nâŒ å·²é‡è¯• {max_retries} æ¬¡ä»ç„¶å¤±è´¥")
                    print("\nðŸ“‹ æ‰‹åŠ¨è§£å†³æ–¹æ¡ˆ:")
                    print("1. è¿è¡Œ: python fix_browser_issue.py")
                    print("2. æ£€æŸ¥ç³»ç»Ÿèµ„æºï¼ˆå†…å­˜ã€ç£ç›˜ç©ºé—´ï¼‰")
                    print("3. é‡å¯ç³»ç»ŸåŽå†è¯•")
                else:
                    print("\nðŸ“‹ å…¶ä»–é”™è¯¯ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿žæŽ¥")
                break
                
        finally:
            try:
                asyncio.get_event_loop().run_until_complete(cleanup())
            except Exception as e:
                print(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    print("è¿›ç¨‹é€€å‡ºï¼Œé€€å‡ºç : -1") 